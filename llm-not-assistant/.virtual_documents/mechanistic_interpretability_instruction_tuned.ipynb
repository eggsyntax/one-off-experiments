





# Install required packages
#!pip install transformer_lens torch matplotlib plotly numpy
# Fix protobuf version conflict that can occur with TransformerLens
#!pip install protobuf==3.20.3
# Optional: Install flash-attention for better performance (if you have CUDA)
# Note: This may fail on some systems - that's okay, the notebook will still work
#!pip install flash-attn --no-build-isolation || echo "flash-attention installation failed - continuing without it"


import torch
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import List, Dict, Tuple, Optional, Callable
import re

from transformer_lens import HookedTransformer, utils
from transformer_lens.hook_points import HookPoint
import torch.nn.functional as F

# Set device
device = torch.device("mps" if torch.mps.is_available() else "cpu")
print(f"Using device: {device}")

# Set up plotting
torch.set_grad_enabled(False)
%matplotlib inline





def extract_chat_template(model: HookedTransformer) -> Tuple[str, Callable, str]:
    """
    Automatically extract the chat template format for any instruction-tuned model.
    
    Returns:
        template_format: Description of the template format
        format_function: Function to format user messages
        assistant_token: The token that represents the assistant role
    """
    tokenizer = model.tokenizer
    
    # Test message for template detection
    test_messages = [
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hi there!"}
    ]
    
    # Method 1: Try HuggingFace's apply_chat_template
    if hasattr(tokenizer, 'apply_chat_template'):
        try:
            # Test with full conversation
            full_template = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=False)
            # Test with just user message + generation prompt
            user_only = tokenizer.apply_chat_template([test_messages[0]], tokenize=False, add_generation_prompt=True)
            
            print(f"Detected chat template via apply_chat_template:")
            print(f"Full conversation: {repr(full_template)}")
            print(f"User + generation prompt: {repr(user_only)}")
            
            # Extract assistant token by finding what was added for generation prompt
            user_without_prompt = tokenizer.apply_chat_template([test_messages[0]], tokenize=False, add_generation_prompt=False)
            assistant_part = user_only.replace(user_without_prompt, "")
            
            def format_function(user_message: str) -> str:
                messages = [{"role": "user", "content": user_message}]
                return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            def format_without_assistant(user_message: str) -> str:
                messages = [{"role": "user", "content": user_message}]
                full_format = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                # Remove the assistant part
                return full_format.replace(assistant_part, "")
            
            return "Auto-detected via apply_chat_template", format_function, format_without_assistant, assistant_part.strip()
        
        except Exception as e:
            print(f"apply_chat_template failed: {e}")
    
    # Method 2: Try to inspect the chat_template attribute
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
        print(f"Found chat_template attribute: {tokenizer.chat_template[:200]}...")
    
    # Method 3: Check special tokens for clues
    special_tokens = tokenizer.special_tokens_map
    all_special = tokenizer.all_special_tokens
    print(f"Special tokens: {special_tokens}")
    print(f"All special tokens: {all_special}")
    
    # Method 4: Model-specific fallbacks based on model name
    model_name = model.cfg.model_name.lower()
    
    if 'phi' in model_name:
        # Phi-3 format: <|user|>\n{message}<|end|>\n<|assistant|>\n
        def format_function(user_message: str) -> str:
            return f"<|user|>\n{user_message}<|end|>\n<|assistant|>\n"
        
        def format_without_assistant(user_message: str) -> str:
            return f"<|user|>\n{user_message}<|end|>\n"
        
        return "Phi-3 format", format_function, format_without_assistant, "<|assistant|>"
    
    elif 'llama' in model_name or 'mistral' in model_name:
        # Llama/Mistral format: [INST] {message} [/INST]
        def format_function(user_message: str) -> str:
            return f"[INST] {user_message} [/INST] "
        
        def format_without_assistant(user_message: str) -> str:
            return f"[INST] {user_message} [/INST]"
        
        return "Llama/Mistral format", format_function, format_without_assistant, " "
    
    elif 'qwen' in model_name:
        # Qwen ChatML format
        def format_function(user_message: str) -> str:
            return f"<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n"
        
        def format_without_assistant(user_message: str) -> str:
            return f"<|im_start|>user\n{user_message}<|im_end|>\n"
        
        return "Qwen ChatML format", format_function, format_without_assistant, "<|im_start|>assistant"
    
    else:
        # Generic ChatML fallback
        def format_function(user_message: str) -> str:
            return f"<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n"
        
        def format_without_assistant(user_message: str) -> str:
            return f"<|im_start|>user\n{user_message}<|im_end|>\n"
        
        return "Generic ChatML format", format_function, format_without_assistant, "<|im_start|>assistant"

def test_template_extraction(model: HookedTransformer, format_func, format_no_assistant, assistant_token):
    """
    Test the extracted template with a simple example
    """
    test_prompt = "Hello, how are you?"
    
    full_format = format_func(test_prompt)
    no_assistant = format_no_assistant(test_prompt)
    
    print(f"\nTemplate testing:")
    print(f"User prompt: {repr(test_prompt)}")
    print(f"Full format: {repr(full_format)}")
    print(f"Without assistant: {repr(no_assistant)}")
    print(f"Assistant token: {repr(assistant_token)}")
    
    # Test tokenization
    full_tokens = model.to_tokens(full_format)
    no_assistant_tokens = model.to_tokens(no_assistant)
    
    print(f"\nTokenization:")
    print(f"Full format tokens: {full_tokens.shape} tokens")
    print(f"Without assistant tokens: {no_assistant_tokens.shape} tokens")
    print(f"Full format: {model.to_str_tokens(full_format)}")
    print(f"Without assistant: {model.to_str_tokens(no_assistant)}")
    
    return full_tokens, no_assistant_tokens





# Load a small instruction-tuned model
# Using Phi-3-mini as it's specifically designed to be research-friendly
model_name = "microsoft/Phi-3-mini-4k-instruct"
print(f"Loading {model_name}...")

# Note: You may see warnings about LayerNorm centering - this is expected
# Phi-3 uses RMSNorm instead of LayerNorm, so TransformerLens skips some 
# interpretability preprocessing steps. This won't affect the notebook's functionality.

model = HookedTransformer.from_pretrained(
    model_name,
    device=device,
    torch_dtype=torch.float16 if device.type == "cuda" else torch.float32
)

print(f"Model loaded! Config: {model.cfg.n_layers} layers, {model.cfg.d_model} dim, {model.cfg.n_heads} heads")
print(f"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")


# Extract the chat template
template_format, format_with_assistant, format_without_assistant, assistant_token = extract_chat_template(model)
print(f"\nExtracted template format: {template_format}")

# Test the template
full_tokens, no_assistant_tokens = test_template_extraction(
    model, format_with_assistant, format_without_assistant, assistant_token
)





# Test prompts
test_prompts = [
    "Hi!",
    "What is the tallest building in the world?",
    "What is 7 + 9?",
    "Tell me a joke.",
    "Explain quantum physics in simple terms."
]

def analyze_assistant_token_prediction(model, prompts, format_with_assistant, format_without_assistant, assistant_token):
    """
    Analyze how strongly the model predicts the assistant token
    """
    results = []
    
    for prompt in prompts:
        # Format without the assistant token
        incomplete_text = format_without_assistant(prompt)
        
        # Tokenize
        tokens = model.to_tokens(incomplete_text)
        
        # Run the model to get logits
        logits = model(tokens)
        
        # Get logits for the next token (what should come after the incomplete text)
        next_token_logits = logits[0, -1, :]  # [vocab_size]
        next_token_probs = F.softmax(next_token_logits, dim=-1)
        
        # Find the assistant token ID using proper tokenizer methods
        assistant_token_ids = []
        try:
            # Try to encode the assistant token directly
            assistant_tokens = model.tokenizer.encode(assistant_token, add_special_tokens=False)
            if assistant_tokens:
                assistant_token_ids.extend(assistant_tokens)
        except:
            pass
        
        # Also try to find tokens that contain the assistant token text
        # We'll search through a sample of the vocabulary
        try:
            vocab_size = model.tokenizer.vocab_size if hasattr(model.tokenizer, 'vocab_size') else len(model.tokenizer)
            # Search through a reasonable range of token IDs
            for token_id in range(min(vocab_size, 50000)):  # Limit search for performance
                try:
                    decoded_token = model.tokenizer.decode([token_id])
                    if assistant_token.lower() in decoded_token.lower() and len(decoded_token.strip()) > 0:
                        if token_id not in assistant_token_ids:
                            assistant_token_ids.append(token_id)
                except:
                    continue
        except:
            pass
        
        # Get the most likely next tokens
        top_k = 10
        top_token_ids = torch.topk(next_token_probs, top_k).indices
        top_probs = torch.topk(next_token_probs, top_k).values
        
        top_tokens = []
        for token_id in top_token_ids:
            try:
                decoded = model.tokenizer.decode([token_id.item()])
                top_tokens.append(decoded)
            except:
                top_tokens.append(f"<token_{token_id.item()}>")
        
        # Calculate assistant token probability
        assistant_prob = sum(next_token_probs[token_id].item() for token_id in assistant_token_ids if token_id < len(next_token_probs))
        
        result = {
            'prompt': prompt,
            'incomplete_text': incomplete_text,
            'assistant_token_prob': assistant_prob,
            'assistant_token_ids': assistant_token_ids,
            'top_tokens': top_tokens,
            'top_probs': top_probs.tolist(),
            'next_token_logits': next_token_logits
        }
        
        results.append(result)
        
        print(f"\nPrompt: {prompt}")
        print(f"Incomplete text ends with: {repr(incomplete_text[-50:])}")
        print(f"Assistant token '{assistant_token}' probability: {assistant_prob:.4f}")
        print(f"Assistant token IDs found: {assistant_token_ids}")
        print(f"Top next tokens: {list(zip(top_tokens, [f'{p:.3f}' for p in top_probs.tolist()]))[:5]}")
    
    return results

# Run the analysis
prediction_results = analyze_assistant_token_prediction(
    model, test_prompts, format_with_assistant, format_without_assistant, assistant_token
)


# Visualize the assistant token prediction strength
prompts = [r['prompt'] for r in prediction_results]
assistant_probs = [r['assistant_token_prob'] for r in prediction_results]

fig = go.Figure(data=go.Bar(
    x=prompts,
    y=assistant_probs,
    text=[f'{p:.3f}' for p in assistant_probs],
    textposition='auto',
))

fig.update_layout(
    title='Assistant Token Prediction Probability by Prompt',
    xaxis_title='Test Prompts',
    yaxis_title='Probability',
    xaxis_tickangle=-45
)

fig.show()

# Print summary statistics
print(f"\nSummary Statistics:")
print(f"Mean assistant token probability: {np.mean(assistant_probs):.4f}")
print(f"Std assistant token probability: {np.std(assistant_probs):.4f}")
print(f"Min assistant token probability: {np.min(assistant_probs):.4f}")
print(f"Max assistant token probability: {np.max(assistant_probs):.4f}")





def create_assistant_token_ablation_hook(model, assistant_token):
    """
    Create a hook that ablates (zeros out) the assistant token's residual stream contribution
    """
    # Find assistant token ID using proper tokenizer methods
    assistant_token_id = None
    try:
        assistant_tokens = model.tokenizer.encode(assistant_token, add_special_tokens=False)
        if assistant_tokens:
            assistant_token_id = assistant_tokens[0]  # Take first token
    except:
        pass
    
    if assistant_token_id is None:
        # Fallback: try to tokenize it with the model
        try:
            assistant_token_id = model.to_tokens(assistant_token)[0, -1].item()
        except:
            assistant_token_id = 0  # Fallback
    
    print(f"Assistant token '{assistant_token}' has ID: {assistant_token_id}")
    
    def ablate_assistant_token(activation, hook):
        # activation shape: [batch, seq_len, d_model]
        # We want to ablate the embedding of the assistant token
        # This is a bit tricky because we need to identify where the assistant token appears
        
        # For now, let's just ablate the embedding contribution
        # In practice, you might want to ablate specific positions
        return activation
    
    return ablate_assistant_token, assistant_token_id

def generate_with_and_without_ablation(model, prompt, format_func, temperature=0.3, max_length=50, num_samples=10):
    """
    Generate completions with and without assistant token ablation
    """
    formatted_prompt = format_func(prompt)
    input_tokens = model.to_tokens(formatted_prompt)
    
    print(f"\nGenerating for prompt: {prompt}")
    print(f"Formatted input: {repr(formatted_prompt)}")
    
    # Generate normal completions
    print(f"\n=== Normal Completions (temperature={temperature}) ===")
    normal_completions = []
    
    for i in range(num_samples):
        completion = model.generate(
            input_tokens,
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=True,
            stop_at_eos=True
        )
        
        # Extract just the new tokens (completion)
        completion_text = model.tokenizer.decode(
            completion[0, input_tokens.shape[1]:], 
            skip_special_tokens=True
        )
        
        normal_completions.append(completion_text)
        print(f"{i+1:2d}: {completion_text[:100]}{'...' if len(completion_text) > 100 else ''}")
    
    # For ablation, we'll try a simpler approach:
    # Generate from the prompt without the assistant token part
    incomplete_prompt = format_without_assistant(prompt)
    incomplete_tokens = model.to_tokens(incomplete_prompt)
    
    print(f"\n=== Ablated Completions (no assistant token, temperature={temperature}) ===")
    print(f"Ablated input: {repr(incomplete_prompt)}")
    
    ablated_completions = []
    
    for i in range(num_samples):
        completion = model.generate(
            incomplete_tokens,
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=True,
            stop_at_eos=True
        )
        
        # Extract just the new tokens (completion)
        completion_text = model.tokenizer.decode(
            completion[0, incomplete_tokens.shape[1]:], 
            skip_special_tokens=True
        )
        
        ablated_completions.append(completion_text)
        print(f"{i+1:2d}: {completion_text[:100]}{'...' if len(completion_text) > 100 else ''}")
    
    return normal_completions, ablated_completions

# Test with a few prompts
test_prompts_ablation = [
    "Hi!",
    "What is 7 + 9?",
    "Tell me a joke."
]

all_results = {}

for prompt in test_prompts_ablation:
    normal, ablated = generate_with_and_without_ablation(
        model, prompt, format_with_assistant, temperature=0.3, max_length=30, num_samples=5
    )
    all_results[prompt] = {'normal': normal, 'ablated': ablated}
    print("\n" + "="*80)





def analyze_completion_differences(all_results):
    """
    Analyze the differences between normal and ablated completions
    """
    print("\n=== Analysis of Completion Differences ===")
    
    for prompt, results in all_results.items():
        normal_completions = results['normal']
        ablated_completions = results['ablated']
        
        print(f"\nPrompt: {prompt}")
        print(f"Normal completions (with assistant token):")
        for i, comp in enumerate(normal_completions[:3]):
            print(f"  {i+1}. {comp[:80]}{'...' if len(comp) > 80 else ''}")
        
        print(f"Ablated completions (without assistant token):")
        for i, comp in enumerate(ablated_completions[:3]):
            print(f"  {i+1}. {comp[:80]}{'...' if len(comp) > 80 else ''}")
        
        # Simple heuristics for comparison
        normal_avg_length = np.mean([len(comp) for comp in normal_completions])
        ablated_avg_length = np.mean([len(comp) for comp in ablated_completions])
        
        print(f"Average completion length - Normal: {normal_avg_length:.1f}, Ablated: {ablated_avg_length:.1f}")
        
        # Check if completions start with typical assistant behaviors
        normal_helpful = sum(1 for comp in normal_completions if any(word in comp.lower()[:50] for word in ['hello', 'hi', 'sure', 'i can', 'the answer']))
        ablated_helpful = sum(1 for comp in ablated_completions if any(word in comp.lower()[:50] for word in ['hello', 'hi', 'sure', 'i can', 'the answer']))
        
        print(f"'Helpful' responses - Normal: {normal_helpful}/{len(normal_completions)}, Ablated: {ablated_helpful}/{len(ablated_completions)}")
        print("-" * 60)

analyze_completion_differences(all_results)



